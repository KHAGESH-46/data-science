
#TASK 1: DATA PIPELINE DEVELOPMENT


import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

#  EXTRACT

def extract_data(file_path):
    print("Extracting data...")

    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print("raw_data.csv not found â€“ creating sample dataset")

        df = pd.DataFrame({
            "age": [25, 30, 35, np.nan, 40],
            "income": [50000, 60000, np.nan, 45000, 80000],
            "city": ["Delhi", "Mumbai", "Chennai", "Bangalore", "Delhi"],
            "purchased": ["Yes", "No", "Yes", "No", "Yes"]
        })

        df.to_csv(file_path, index=False)
        print("Sample raw_data.csv created")

    return df



# 2. TRANSFORM

def transform_data(df):
    print("Transforming data...")

    # Separate features and target
    X = df.drop(columns=["purchased"])
    y = df["purchased"]

    # Identify column types
    numerical_cols = X.select_dtypes(include=["int64", "float64"]).columns
    categorical_cols = X.select_dtypes(include=["object"]).columns

    # Numerical pipeline
    numerical_pipeline = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="mean")),
        ("scaler", StandardScaler())
    ])

    # Categorical pipeline
    categorical_pipeline = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("encoder", OneHotEncoder(handle_unknown="ignore"))
    ])

    # Combine pipelines
    preprocessor = ColumnTransformer(transformers=[
        ("num", numerical_pipeline, numerical_cols),
        ("cat", categorical_pipeline, categorical_cols)
    ])

    # Apply transformations
    X_processed = preprocessor.fit_transform(X)

    return X_processed, y, preprocessor


# 3. LOAD

def load_data(X, y, output_path):
    print("Loading processed data...")

    X_df = pd.DataFrame(X.toarray() if hasattr(X, "toarray") else X)
    y_df = pd.DataFrame(y, columns=["purchased"])

    final_df = pd.concat([X_df, y_df.reset_index(drop=True)], axis=1)
    final_df.to_csv(output_path, index=False)

    print(f"Processed data saved to {output_path}")


# MAIN PIPELINE

def run_pipeline():
    input_file = "raw_data.csv"
    output_file = "processed_data.csv"

    df = extract_data(input_file)
    X_processed, y, preprocessor = transform_data(df)
    load_data(X_processed, y, output_file)

    print("ETL Pipeline completed successfully!")


# RUN
if __name__ == "__main__":
    run_pipeline()
